For a fixed integer~$N\ge 1$ and a fixed language,
an {\em $N$-gram model} of that language
is simply a function that maps any $N$-tuple
$\left(w_1,w_2,\ldots,w_N\right)$ of words in that
language to the probability that
$w_N$ could appear immediately after $w_1w_2\cdots w_{N-1}$
in exactly that order in a sentence in the language.
The model is usually denoted by
$P\left(w_N\;\vert\; w_1,w_2,\ldots,w_{N-1}\right)$
with the understanding
that $w_1,w_2,\ldots,w_N$ are the arguments of the function.

One approximates
$P\left(w_N\;\vert\; w_1,w_2,\ldots,w_{N-1}\right)$
as the ratio of the number of occurrences
of $w_1w_2\cdots w_N$ by the number of occurrences
of $w_1w_2\cdots w_{N-1}$ in a fixed training corpus.
Note that this calculation depends heavily on the training corpus,
so it becomes important for the corpus to reflect the style
of writing one is trying to model.

One refers to 
$P\left(w_N\;\vert\; w_1,w_2,\ldots,w_{N-1}\right)$
as a {\em grammar} because it asserts which sequences
of words should be considered legitimate sentences in the language,
in the same way that grammar in the usual sense, with its subjects
and predicates and such, dictates how 
sentences are formed,
and thereby which sequences of words
qualify as legitimate sentences.
It does this by simply assigning probabilities to every sequence
of words; sequences whose probability falls below some threshold
fail to be valid sentences in the language.

One implements
$P\left(w_N\;\vert\; w_1,w_2,\ldots,w_{N-1}\right)$
as a finite state machine $G$.
Considering the bigraphic model, we take the states
of $G$ to be the vocabulary words, and for each pair of words $w,x$
we introduce a transition $w\to x$ labeled by ${x:x/w}$ where
$w=\ln\left(P\left(x\;\vert\; w\right)\right)$.
In this way, a string of words will be accepted if there is a path
through $G$ passing through the words of the sentence, in the same order.
Furthermore, the weight of this path approximately
gives the log of the probability of the string.
