For a fixed natural number~$N$, an {\em $N$-gram language model}
is simply a function that maps any $N$~words
$w_1,w_2,\ldots,w_N$ to the probability of $w_N$ appearing
immediately after a consecutive occurrence of $w_1w_2\cdots w_{N-1}$,
in exactly that order.
The model is usually denoted by
$P\left(w_N\vert w_1,w_2,\ldots,w_{N_1}\right)$,
with the understanding
that $w_1,w_2,\ldots,w_N$ are the arguments of the function.

One approximates
$P\left(w_N\vert w_1,w_2,\ldots,w_{N_1}\right)$
as the ratio of the number of occurrences
of $w_1w_2\cdots w_N$ by the number of occurrences
of $w_1w_2\cdots w_{N_1}$ in a fixed training corpus.
Note that this calculation depends strongly on the training corpus,
so it becomes important for the corpus to reflect the style
of writing one is trying to model.

One refers to 
$P\left(w_N\vert w_1,w_2,\ldots,w_{N_1}\right)$
as a {\em grammar} inasmuch as it communicates which sequences
of words are to be considered valid sentences in the language
being modeled by simply assigning probabilities to any sequence
of words. Word sequences with low probability might fail to be
valid sentences in the language.
Note that 
$P\left(w_N\vert w_1,w_2,\ldots,w_{N_1}\right)$
knows knothing about subjects, predicates, and such.

One implements
$P\left(w_N\vert w_1,w_2,\ldots,w_{N_1}\right)$
as a finite state machine $G$.
Considering the bigraphic model, we take the states
of $G$ to be the vocabulary words, and for each pair of words $w,x$
we introduce a transition $w\to x$ labeled by ${x:x/w}$ where
$w=\ln\left(P\left(x\vert w\right)\right)$.
In this way, a string of words will be accepted if there is a path
through $G$ passing through the words of the sentence, in the same order.
Furthermore, the weight of this path approximately
gives the log of the probability of the string.
